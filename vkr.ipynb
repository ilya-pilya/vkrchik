{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 23:55:15.930 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "import joblib\n",
    "import streamlit as st\n",
    "import docx\n",
    "import fitz\n",
    "import PyPDF2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, scrolledtext\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from deep_translator import GoogleTranslator\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "st.set_page_config(page_title='Определение языка текста', layout='centered')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "directory = \"C:/Users/pillya/Documents/ish/vkr/datasets\"\n",
    "model_path = 'language_classifier.pkl'\n",
    "vectorizer_path = 'tfidf_vectorizer.pkl'\n",
    "\n",
    "def convert_tsv_to_csv(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.tsv'):\n",
    "            tsv_file = os.path.join(directory, filename)\n",
    "            csv_file = os.path.join(directory, filename.replace('.tsv', '.csv'))\n",
    "            try:\n",
    "                with open(tsv_file, 'r', newline='', encoding='utf-8') as infile, open(csv_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "                    reader = csv.reader(infile, delimiter='\\t')\n",
    "                    writer = csv.writer(outfile)\n",
    "                    for row in reader:\n",
    "                        writer.writerow(row)\n",
    "                logging.info(f'Converted {tsv_file} to {csv_file}')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error converting {tsv_file}: {e}\")\n",
    "\n",
    "def process_csv_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, header=None)\n",
    "                df.columns = ['text', 'value']\n",
    "                df.to_csv(file_path, index=False)\n",
    "                logging.info(f'Successfully processed and overwritten {filename}')\n",
    "            except PermissionError as e:\n",
    "                logging.error(f\"Permission error for {filename}: {e}\")\n",
    "            except FileNotFoundError as e:\n",
    "                logging.error(f\"File not found: {e}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An error occurred while processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 23:55:16,002 - INFO - Converted C:/Users/pillya/Documents/ish/vkr/datasets\\ger.tsv to C:/Users/pillya/Documents/ish/vkr/datasets\\ger.csv\n",
      "2025-05-03 23:55:16,026 - INFO - Converted C:/Users/pillya/Documents/ish/vkr/datasets\\gre.tsv to C:/Users/pillya/Documents/ish/vkr/datasets\\gre.csv\n",
      "2025-05-03 23:55:16,040 - INFO - Converted C:/Users/pillya/Documents/ish/vkr/datasets\\rus.tsv to C:/Users/pillya/Documents/ish/vkr/datasets\\rus.csv\n",
      "2025-05-03 23:55:16,055 - INFO - Converted C:/Users/pillya/Documents/ish/vkr/datasets\\tur.tsv to C:/Users/pillya/Documents/ish/vkr/datasets\\tur.csv\n",
      "2025-05-03 23:55:16,097 - INFO - Successfully processed and overwritten ger.csv\n",
      "2025-05-03 23:55:16,135 - INFO - Successfully processed and overwritten gre.csv\n",
      "2025-05-03 23:55:16,157 - INFO - Successfully processed and overwritten rus.csv\n",
      "2025-05-03 23:55:16,177 - INFO - Successfully processed and overwritten tur.csv\n",
      "2025-05-03 23:55:16,256 - INFO - Очистка прошла успешно C:/Users/pillya/Documents/ish/vkr/datasets\\tur.csv (строк после сэмплирования: 5000)\n",
      "2025-05-03 23:55:16,339 - INFO - Очистка прошла успешно C:/Users/pillya/Documents/ish/vkr/datasets\\rus.csv (строк после сэмплирования: 5000)\n",
      "2025-05-03 23:55:16,355 - INFO - Очистка прошла успешно C:/Users/pillya/Documents/ish/vkr/datasets\\gre.csv (строк после сэмплирования: 5000)\n",
      "2025-05-03 23:55:16,376 - INFO - Очистка прошла успешно C:/Users/pillya/Documents/ish/vkr/datasets\\ger.csv (строк после сэмплирования: 5000)\n",
      "2025-05-03 23:55:16,389 - INFO - Баланс классов в обучающем наборе:\n",
      "2025-05-03 23:55:16,390 - INFO - language\n",
      "tur    4000\n",
      "ger    4000\n",
      "gre    4000\n",
      "rus    4000\n",
      "Name: count, dtype: int64\n",
      "2025-05-03 23:55:16,539 - INFO - Векторизация с символьными n-граммами выполнена успешно\n",
      "2025-05-03 23:55:36,620 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "         ger       0.90      0.86      0.88      1000\n",
      "         gre       0.97      0.86      0.91      1000\n",
      "         rus       0.85      0.88      0.87      1000\n",
      "         tur       0.81      0.90      0.85      1000\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.88      0.88      0.88      4000\n",
      "weighted avg       0.88      0.88      0.88      4000\n",
      "\n",
      "C:\\Users\\pillya\\AppData\\Local\\Temp\\ipykernel_7724\\2839367693.py:85: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите несколько слов на любом языке, и модель предскажет язык:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 23:55:40,205 - INFO -   language            text  value\n",
      "0      ger  Servicepartner      O\n",
      "1      ger          Moment      O\n",
      "2      ger            Satz  I-LAW\n",
      "3      ger             von      O\n",
      "4      ger           nehme      O\n",
      "5      ger      lückenhaft      O\n",
      "6      ger            dass      O\n",
      "7      ger   tatsächlichen      O\n",
      "8      ger             der      O\n",
      "9      ger        gleichen      O\n",
      "2025-05-03 23:55:40,207 - INFO - language\n",
      "ger    5000\n",
      "gre    5000\n",
      "tur    5000\n",
      "rus    5000\n",
      "Name: count, dtype: int64\n",
      "2025-05-03 23:55:40,208 - INFO - [[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанный язык: rus\n",
      "Топ n-грамм для русского языка:\n",
      "о\n",
      ",\n",
      "и\n",
      "в\n",
      "а\n",
      "е\n",
      "н\n",
      "с\n",
      ".\n",
      "т\n",
      "р\n",
      "л\n",
      "к\n",
      "д\n",
      "м\n",
      "п\n",
      "у\n",
      "я\n",
      "б\n",
      "на\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 23:55:41,012 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "         ger       0.90      0.86      0.88      1000\n",
      "         gre       0.97      0.86      0.91      1000\n",
      "         rus       0.85      0.88      0.87      1000\n",
      "         tur       0.81      0.90      0.85      1000\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.88      0.88      0.88      4000\n",
      "weighted avg       0.88      0.88      0.88      4000\n",
      "\n",
      "2025-05-03 23:55:41,014 - INFO - Text: başına\n",
      "True Language: tur\n",
      "Predicted Language: tur\n",
      "\n",
      "2025-05-03 23:55:41,016 - INFO - Text: 3\n",
      "True Language: ger\n",
      "Predicted Language: ger\n",
      "\n",
      "2025-05-03 23:55:41,018 - INFO - Text: Турецкой\n",
      "True Language: rus\n",
      "Predicted Language: rus\n",
      "\n",
      "2025-05-03 23:55:41,020 - INFO - Text: fünf\n",
      "True Language: ger\n",
      "Predicted Language: ger\n",
      "\n",
      "2025-05-03 23:55:41,022 - INFO - Text: όσοι\n",
      "True Language: gre\n",
      "Predicted Language: gre\n",
      "\n",
      "2025-05-03 23:55:41,024 - INFO - Предсказанный язык: rus\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def clean_dataset(df):\n",
    "    df = df.dropna(subset=['text', 'value'])\n",
    "    df = df[df['text'].str.strip() != '']\n",
    "    df = df[df['value'].str.strip() != '']\n",
    "    df = df[df['text'].apply(lambda x: all(c.isprintable() for c in x))]\n",
    "    df = df[df['value'].apply(lambda x: all(c.isprintable() for c in x))]\n",
    "    return df\n",
    "\n",
    "def load_and_clean_datasets(directory, languages):\n",
    "    datasets = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(load_and_clean_dataset, directory, lang) for lang in languages]\n",
    "        for future in futures:\n",
    "            df = future.result()\n",
    "            if df is not None:\n",
    "                datasets.append(df)\n",
    "    return datasets\n",
    "\n",
    "def load_and_clean_dataset(directory, lang):\n",
    "    file_path = os.path.join(directory, f'{lang}.csv')\n",
    "    try:\n",
    "        df = load_dataset(file_path)\n",
    "        df = clean_dataset(df)\n",
    "        df = df.sample(n=min(5000, len(df)), random_state=42)   #Ограничение\n",
    "        logging.info(f'Очистка прошла успешно {file_path} (строк после сэмплирования: {len(df)})')\n",
    "        return df\n",
    "    except PermissionError as e:\n",
    "        logging.error(f\"Permission error: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    convert_tsv_to_csv(directory)\n",
    "    process_csv_files(directory)\n",
    "\n",
    "    languages = ['ger', 'gre', 'tur', 'rus']\n",
    "    datasets = load_and_clean_datasets(directory, languages)\n",
    "\n",
    "    combined_df = pd.concat(datasets, keys=languages, names=['language', 'index'])\n",
    "    combined_df = combined_df.reset_index(level=1, drop=True).reset_index()\n",
    "\n",
    "    train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42, stratify=combined_df['language'])\n",
    "\n",
    "    if '1.' in train_df.columns:\n",
    "        train_df.rename(columns={'1.': 'text'}, inplace=True)\n",
    "    if '1.' in test_df.columns:\n",
    "        test_df.rename(columns={'1.': 'text'}, inplace=True)\n",
    "    if 'text' not in train_df.columns:\n",
    "        raise KeyError(\"Столбец 'text' не найден в train_df\")\n",
    "    if 'text' not in test_df.columns:\n",
    "        raise KeyError(\"Столбец 'text' не найден в test_df\")\n",
    "\n",
    "    logging.info(\"Баланс классов в обучающем наборе:\")\n",
    "    logging.info(train_df['language'].value_counts())\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 4), max_features=5000)\n",
    "    X_train = vectorizer.fit_transform(train_df['text'].dropna())\n",
    "    X_test = vectorizer.transform(test_df['text'].dropna())\n",
    "    y_train = train_df['language']\n",
    "    y_test = test_df['language']\n",
    "    logging.info(\"Векторизация с символьными n-граммами выполнена успешно\")\n",
    "\n",
    "    model = VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('svc', SVC(probability=True)),\n",
    "        ('nb', MultinomialNB())\n",
    "    ], voting='soft')\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    logging.info(classification_report(y_test, y_pred))\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=languages, yticklabels=languages)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Матрица ошибок')\n",
    "    plt.show()\n",
    "\n",
    "    joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "    joblib.dump(model, 'language_classifier.pkl')\n",
    "\n",
    "    #vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "    #model = joblib.load('language_classifier.pkl')\n",
    "    \n",
    "    def predict_language(text):\n",
    "        text_vectorized = vectorizer.transform([text])\n",
    "        predicted_language = model.predict(text_vectorized)[0]\n",
    "        return predicted_language\n",
    "\n",
    "    print(\"Введите несколько слов на любом языке, и модель предскажет язык:\")\n",
    "    user_input = input(\"Ввод: \")\n",
    "    predicted_language = predict_language(user_input)\n",
    "    print(f\"Предсказанный язык: {predicted_language}\")\n",
    "\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    rus_texts = train_df[train_df['language'] == 'rus']['text']\n",
    "    rus_vectorized = vectorizer.transform(rus_texts)\n",
    "    mean_tfidf = rus_vectorized.mean(axis=0).A1\n",
    "    top_indices = mean_tfidf.argsort()[-20:][::-1]\n",
    "    print(\"Топ n-грамм для русского языка:\")\n",
    "    for i in top_indices:\n",
    "        print(features[i])\n",
    "\n",
    "    # Проверка данных\n",
    "    logging.info(combined_df.head(10))\n",
    "    logging.info(combined_df['language'].value_counts())\n",
    "    sample_text = \"Доброе утро\"\n",
    "    sample_vectorized = vectorizer.transform([sample_text])\n",
    "    logging.info(sample_vectorized.toarray())\n",
    "    y_pred = model.predict(X_test)\n",
    "    logging.info(classification_report(y_test, y_pred))\n",
    "    for i in range(5):\n",
    "        sample_text = test_df['text'].iloc[i]\n",
    "        sample_vectorized = vectorizer.transform([sample_text])\n",
    "        predicted_language = model.predict(sample_vectorized)[0]\n",
    "        true_language = test_df['language'].iloc[i]\n",
    "        logging.info(f\"Text: {sample_text}\\nTrue Language: {true_language}\\nPredicted Language: {predicted_language}\\n\")\n",
    "\n",
    "    user_input = \"Россия\"\n",
    "    predicted_language = predict_language(user_input)\n",
    "    logging.info(f\"Предсказанный язык: {predicted_language}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_map = {\n",
    "    'ger': 'de',\n",
    "    'gre': 'el',\n",
    "    'tur': 'tr',\n",
    "    'rus': 'ru'\n",
    "}\n",
    "\n",
    "lang_names = {\n",
    "    'de': 'Немецкий',\n",
    "    'el': 'Греческий',\n",
    "    'tr': 'Турецкий',\n",
    "    'ru': 'Русский'\n",
    "}\n",
    "\n",
    "vectorizer = joblib.load(vectorizer_path)\n",
    "model = joblib.load(model_path)\n",
    "file_path = 'C:/Users/pillya/Documents/ish/vkr/sosiski.txt'\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == '.txt':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "        \n",
    "    elif ext == '.pdf':\n",
    "        text = \"\"\n",
    "        with fitz.open(file_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "    \n",
    "    elif ext == '.docx':\n",
    "        doc = Document(file_path)\n",
    "        return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Формат файла {ext} не поддерживается.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language_from_file(text):\n",
    "    text = extract_text_from_file(file_path)\n",
    "    if not text.strip():\n",
    "        raise ValueError(\"Файл пустой или не удалось извлечь текст.\")\n",
    "    \n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    prediction = model.predict(text_vectorized)[0]\n",
    "    return prediction\n",
    "\n",
    "def translate_text(text, source_lang_code, target_langs):\n",
    "    translations = {}\n",
    "    for target_lang_code in target_langs:\n",
    "        try:\n",
    "            translated = GoogleTranslator(source=source_lang_code, target=target_lang_code).translate(text)\n",
    "            translations[target_lang_code] = translated\n",
    "        except Exception as e:\n",
    "            translations[target_lang_code] = f\"Ошибка перевода: {e}\"\n",
    "    return translations\n",
    "\n",
    "# try:\n",
    "#     text = extract_text_from_file(file_path)\n",
    "#     print(\"Текст успешно извлечён.\")\n",
    "#     predicted_label = detect_language_from_file(text)\n",
    "#     source_lang_code = lang_map[predicted_label]\n",
    "#     print(f\"Определённый язык: {predicted_label.upper()} ({source_lang_code})\")\n",
    "\n",
    "#     translations = translate_text(text, source_lang_code, target_langs)\n",
    "#     for lang, translated_text in translations.items():\n",
    "#         print(f\"\\nПеревод на {lang.upper()}:\")\n",
    "#         print(translated_text[:1000])\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if not file_path:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        text = extract_text_from_file(file_path)\n",
    "        lang_label = detect_language_from_file(text)\n",
    "        source_lang_code = lang_map[lang_label]\n",
    "\n",
    "        result_text = f\"Определённый язык: {lang_label.upper()} ({lang_names[source_lang_code]})\\n\\n\"\n",
    "\n",
    "        selected_langs = [code for code, var in lang_vars.items() if var.get()]\n",
    "        if source_lang_code in selected_langs:\n",
    "            selected_langs.remove(source_lang_code)\n",
    "\n",
    "        if not selected_langs:\n",
    "            messagebox.showinfo(\"Информация\", \"Выберите хотя бы один язык(кроме исходного)\")\n",
    "            return\n",
    "        \n",
    "        translations = translate_text(text, source_lang_code, selected_langs)\n",
    "        for code, translated in translations.items():\n",
    "            result_text += f\"--- Перевод на {lang_names[code]} ({code.upper()}) ---\\n{translated[:1000]}\\n\\n\"\n",
    "\n",
    "        result_box.delete(1.0, tk.END)\n",
    "        result_box.insert(tk.END, result_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Ошибка\", f\"Не удалось обработать файл:\\n{e}\")\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Определение языка и перевод\")\n",
    "root.geometry(\"850x700\")\n",
    "\n",
    "frame = tk.Frame(root)\n",
    "frame.pack(pady=10)\n",
    "\n",
    "browse_button = tk.Button(frame, text=\"Выбрать файл\", command=browse_file)\n",
    "browse_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "lang_vars = {}\n",
    "for code, name in lang_names.items():\n",
    "    var = tk.BooleanVar(value=True)\n",
    "    cb = tk.Checkbutton(frame, text=name, variable=var)\n",
    "    cb.pack(side=tk.LEFT)\n",
    "    lang_vars[code] = var\n",
    "\n",
    "result_box = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=100, height=30)\n",
    "result_box.pack(padx=10, pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
